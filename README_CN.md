# 一个可解释的分布式存储核心参考实现

> **面向系统工程师的最小分布式存储内核（Reference Implementation）**

---

## 这是什么？

这是一个**分布式存储核心的参考实现**，用于回答一个被长期忽视、但极其关键的问题：

> **构建一个“正确”的分布式存储系统，最小需要哪些机制？**

本项目刻意避免规模化、产品化和特性堆叠，而是专注于：

- 正确性（Correctness）
- 可解释性（Explainability）
- 可运行、可验证的工程闭环

它适合被阅读、被调试、被修改，而不是被直接部署到生产环境。

---

## 为什么需要这样的项目？

当下大多数分布式存储系统（如 Ceph、TiKV、HDFS）：

- 体量巨大，学习曲线陡峭
- 正确性逻辑被淹没在工程复杂度中
- 很难回答“**为什么必须这样设计**”

而教学型 Demo 又往往：

- 忽略真实的崩溃/故障语义
- 不能运行或无法复现真实失败场景

本项目位于两者之间：

> **一个最小但真实的分布式存储系统内核。**

---

## 项目定位（明确边界）

这是一个：

- ✔ 分布式存储**核心机制**的参考实现
- ✔ 可以 crash、failover、恢复 的系统
- ✔ 为系统工程师提供“可验证心智模型”的代码库

而**不是**：

- ❌ 一个生产级存储产品
- ❌ 一个完整的 KV / Object Store
- ❌ 一个高性能或高扩展性系统

---

## 核心设计思想

### 1. 将“崩溃一致性”和“分布式一致性”解耦

- 单机层面：
  - 使用 WAL 保证崩溃恢复正确性
- 集群层面：
  - 使用 Primary–Backup + Epoch fencing 防止脑裂

这两类问题**被刻意分离建模**，便于理解与验证。

### 2. 明确“唯一事实来源”

- 单机恢复阶段：WAL 是唯一事实来源
- 集群层面：epoch 决定写入合法性

系统中不存在“隐含状态”或“魔法恢复路径”。

### 3. 明确哪些问题不在当前讨论范围内

复杂机制（如 Raft、Compaction、GC）并非被忽略，  
而是被**有意识地延后**，以避免干扰核心因果关系。

---

## 系统整体结构

```
Client
  |
  v
Gateway（路由 / 重试 / failover 触发）
  |
  v
Primary StorageNode  <---->  Backup StorageNodes
  |
  v
DiskEngine（WAL + Allocator + Data Records）
```

---

## DiskEngine：单机存储内核

DiskEngine 是一个独立的 C 语言存储引擎，用于展示：

- 明确的 on-disk layout
- WAL 驱动的崩溃一致性
- 可验证的恢复过程

### On-Disk Layout

```
| Superblock A | Superblock B | WAL Region | Data Region |
```

关键点：

- Superblock 使用 A/B 双写 + epoch
- WAL 是崩溃恢复阶段的唯一事实来源
- Data record 不可变，读路径带 CRC 校验

---

## Dataplane：最小分布式数据路径

Dataplane 提供：

- 明确的 Primary / Backup 角色
- 顺序化写入与复制
- Epoch fencing 防止脑裂
- Primary 崩溃后的显式 failover

该实现的目标是：

> **展示“最小但真实”的分布式写路径，而非极致性能。**

---

## 如何使用这个项目

### 你可以：

- 阅读代码，理解每一个状态转移
- 手动 kill primary，观察 failover 行为
- 修改代码，引入 checkpoint / GC / Raft 等机制

### 你不应该：

- 将其直接用于生产
- 将其与成熟产品做性能对比

---

## 项目成熟度

这是一个**已完成的参考级实现**：

- 核心数据路径闭环
- 崩溃与 failover 语义自洽
- 架构边界清晰、可扩展

后续工作属于**研究与演进**，而非修补缺陷。

---

## 适合谁？

- 分布式系统 / 存储系统工程师
- 希望建立“正确性优先”心智模型的学习者
- 需要一个可运行参考内核的开源研究者
